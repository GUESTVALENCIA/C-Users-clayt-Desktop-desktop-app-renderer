// qwen-intelligent-integration.js - Integraci√≥n inteligente de Qwen con selecci√≥n autom√°tica de modelos

// Cargar el selector de modelos
const { modelSelector, selectModelForInput, detectInputType } = require('./model-selector');

// Clase principal de integraci√≥n inteligente
class QwenIntelligentIntegration {
  constructor() {
    this.ollamaUrl = 'http://localhost:11434';
    this.currentModel = 'qwen2.5:7b';
    this.isConnected = false;
    this.chatHistory = [];
    this.tokenUsage = {};
    
    // Verificar conexi√≥n al inicializar
    this.checkConnection();
  }

  // Verificar si Ollama est√° disponible
  async checkConnection() {
    try {
      const response = await fetch(`${this.ollamaUrl}/api/tags`);
      this.isConnected = response.ok;
      
      if (this.isConnected) {
        console.log('‚úÖ Conexi√≥n con Qwen a trav√©s de Ollama establecida');
        
        // Emitir evento de conexi√≥n exitosa
        if (window && window.dispatchEvent) {
          window.dispatchEvent(new CustomEvent('qwenConnectionStatus', {
            detail: { connected: true, model: this.currentModel }
          }));
        }
      } else {
        console.log('‚ùå No se puede conectar con Ollama');
      }
      
      return this.isConnected;
    } catch (error) {
      console.error('Error verificando conexi√≥n con Ollama:', error);
      this.isConnected = false;
      return false;
    }
  }

  // Seleccionar modelo inteligentemente basado en la entrada
  selectModelForInput(input, requirements = {}) {
    // Detectar el tipo de entrada
    const inputAnalysis = detectInputType(input);
    
    // Seleccionar modelo basado en el an√°lisis
    const selectedModel = selectModelForInput(input, {
      ...requirements,
      taskType: inputAnalysis.taskType,
      hasImage: inputAnalysis.hasImage
    });
    
    console.log(`ü§ñ Modelo seleccionado: ${selectedModel.name} (Tarea: ${inputAnalysis.taskType})`);
    
    // Actualizar modelo actual
    this.currentModel = selectedModel.name;
    
    return selectedModel;
  }

  // Enviar mensaje a Qwen con selecci√≥n inteligente de modelo
  async sendMessage(message, options = {}) {
    if (!this.isConnected) {
      await this.checkConnection();
      if (!this.isConnected) {
        throw new Error('No hay conexi√≥n con Ollama');
      }
    }

    try {
      // Seleccionar modelo inteligentemente
      const selectedModel = this.selectModelForInput(message, options);
      
      // A√±adir mensaje del usuario al historial
      this.chatHistory.push({ role: 'user', content: message });
      
      const response = await fetch(`${this.ollamaUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: selectedModel.name,
          messages: this.chatHistory,
          stream: false,
          options: {
            temperature: options.temperature || 0.7,
            max_tokens: options.max_tokens || 2048,
            keep_alive: options.keep_alive || '5m'
          }
        })
      });

      if (!response.ok) {
        throw new Error(`Error de Ollama: ${response.status} ${response.statusText}`);
      }

      const data = await response.json();
      const assistantResponse = data.message.content;
      
      // A√±adir respuesta del asistente al historial
      this.chatHistory.push({ role: 'assistant', content: assistantResponse });
      
      // Calcular uso de tokens (aproximado)
      const tokensUsed = Math.ceil(assistantResponse.length / 4); // Aproximaci√≥n grosera
      modelSelector.updateTokenUsage(selectedModel.name, tokensUsed);
      
      // Mantener solo los √∫ltimos 20 mensajes para no sobrecargar la memoria
      if (this.chatHistory.length > 20) {
        this.chatHistory = this.chatHistory.slice(-20);
      }
      
      return {
        response: assistantResponse,
        modelUsed: selectedModel.name,
        taskType: detectInputType(message).taskType,
        tokensUsed: tokensUsed
      };
    } catch (error) {
      console.error('Error al comunicarse con Qwen:', error);
      throw error;
    }
  }

  // Enviar mensaje con imagen (si se detecta imagen en el input)
  async sendMessageWithImage(imageData, message = '') {
    if (!this.isConnected) {
      await this.checkConnection();
      if (!this.isConnected) {
        throw new Error('No hay conexi√≥n con Ollama');
      }
    }

    // Seleccionar modelo de visi√≥n
    const visionModel = modelSelector.getVisionModels()[0];
    if (!visionModel) {
      throw new Error('No hay modelos de visi√≥n disponibles');
    }

    try {
      const response = await fetch(`${this.ollamaUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: visionModel.name,
          prompt: message,
          images: [imageData], // imageData debe ser una cadena base64
          stream: false,
          options: {
            temperature: 0.7,
            max_tokens: 2048
          }
        })
      });

      if (!response.ok) {
        throw new Error(`Error de Ollama: ${response.status} ${response.statusText}`);
      }

      const data = await response.json();
      const assistantResponse = data.response;
      
      // A√±adir al historial
      this.chatHistory.push({ 
        role: 'user', 
        content: message,
        images: [imageData]
      });
      this.chatHistory.push({ role: 'assistant', content: assistantResponse });
      
      // Calcular uso de tokens
      const tokensUsed = Math.ceil(assistantResponse.length / 4);
      modelSelector.updateTokenUsage(visionModel.name, tokensUsed);
      
      return {
        response: assistantResponse,
        modelUsed: visionModel.name,
        taskType: 'vision',
        tokensUsed: tokensUsed
      };
    } catch (error) {
      console.error('Error al comunicarse con Qwen (imagen):', error);
      throw error;
    }
  }

  // Reiniciar historial de chat
  resetChatHistory() {
    this.chatHistory = [];
  }

  // Obtener modelos disponibles
  async getAvailableModels() {
    try {
      const response = await fetch(`${this.ollamaUrl}/api/tags`);
      if (!response.ok) {
        throw new Error(`Error al obtener modelos: ${response.status}`);
      }
      
      const data = await response.json();
      return data.models || [];
    } catch (error) {
      console.error('Error obteniendo modelos:', error);
      return [];
    }
  }

  // Obtener informaci√≥n sobre el uso de tokens
  getTokenUsage() {
    return this.tokenUsage;
  }

  // Cargar contexto desde memoria persistente
  async loadContext() {
    try {
      // Intentar cargar desde el sistema de archivos si est√° disponible
      if (window && window.sandra && window.sandra.fs) {
        const context = await window.sandra.fs.loadQwenContext();
        if (context && context.chatHistory) {
          this.chatHistory = context.chatHistory;
        }
      }
    } catch (error) {
      console.warn('No se pudo cargar contexto previo:', error);
    }
  }

  // Guardar contexto en memoria persistente
  async saveContext() {
    try {
      // Guardar en el sistema de archivos si est√° disponible
      if (window && window.sandra && window.sandra.fs) {
        await window.sandra.fs.saveQwenContext({
          chatHistory: this.chatHistory,
          model: this.currentModel,
          timestamp: Date.now(),
          tokenUsage: this.tokenUsage
        });
      }
    } catch (error) {
      console.warn('No se pudo guardar contexto:', error);
    }
  }
}

// Inicializar la integraci√≥n cuando el DOM est√© listo
let qwenIntegration = null;

if (document.readyState === 'loading') {
  document.addEventListener('DOMContentLoaded', () => {
    qwenIntegration = new QwenIntelligentIntegration();
  });
} else {
  qwenIntegration = new QwenIntelligentIntegration();
}

// Hacer disponible globalmente para otros m√≥dulos
window.QwenIntelligentIntegration = QwenIntelligentIntegration;
window.qwenIntegration = qwenIntegration;

// Funci√≥n para enviar mensaje directamente con selecci√≥n inteligente
window.sendQwenMessage = async function(message, options = {}) {
  if (!qwenIntegration) {
    qwenIntegration = new QwenIntelligentIntegration();
  }
  
  return await qwenIntegration.sendMessage(message, options);
};

// Funci√≥n para enviar mensaje con imagen
window.sendQwenMessageWithImage = async function(imageData, message = '') {
  if (!qwenIntegration) {
    qwenIntegration = new QwenIntelligentIntegration();
  }
  
  return await qwenIntegration.sendMessageWithImage(imageData, message);
};

// Funci√≥n para verificar estado de conexi√≥n
window.checkQwenConnection = async function() {
  if (!qwenIntegration) {
    qwenIntegration = new QwenIntelligentIntegration();
  }
  
  return await qwenIntegration.checkConnection();
};

// Funci√≥n para seleccionar modelo inteligentemente
window.selectModelForInput = function(input, requirements = {}) {
  return qwenIntegration.selectModelForInput(input, requirements);
};

// Funci√≥n para detectar tipo de entrada
window.detectInputType = function(input) {
  return detectInputType(input);
};

console.log('‚úÖ Integraci√≥n inteligente de Qwen inicializada');
console.log('‚úÖ Disponible: window.qwenIntegration, window.sendQwenMessage, window.selectModelForInput');

// Exportar para uso en m√≥dulos Node.js si es necesario
if (typeof module !== 'undefined' && module.exports) {
  module.exports = { QwenIntelligentIntegration, qwenIntegration };
}